{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import dateutil\n",
    "from scipy.sparse import lil_matrix # To build sparse feature matrices, if you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "f = gzip.open(\"steam_category.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntrain = 10000\n",
    "Ntest = 10000\n",
    "\n",
    "dataTrain = dataset[:Ntrain]\n",
    "dataTest = dataset[Ntrain:Ntrain + Ntest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = set(string.punctuation)\n",
    "word_counts = Counter()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    return ''.join([c for c in text.lower() if c not in sp])\n",
    "\n",
    "for review in dataTrain:\n",
    "    text = preprocess_text(review['text'])\n",
    "    words = text.split()\n",
    "    word_counts.update(words)\n",
    "\n",
    "counts = word_counts.most_common(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = [(count, word) for word, count in counts[:10]]\n",
    "assertFloatList([x[0] for x in answers['Q1']], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "NW = 1000 # dictionary size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word, _ in counts[:NW]]  # Top 1000 words from Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build X...\n",
    "vectorizer = CountVectorizer(vocabulary=words)  # Use the top 1000 words as the vocabulary\n",
    "X = vectorizer.fit_transform([review['text'].lower() for review in dataset])  # Convert reviews to feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [review['genreID'] for review in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(Xtrain, ytrain)\n",
    "predictions = mod.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = [predictions[i] == ytest[i] for i in range(len(ytest))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = sum(correct) / len(correct)\n",
    "assertFloat(answers['Q2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_words = [\"character\", \"game\", \"length\", \"a\", \"it\"]\n",
    "\n",
    "df = defaultdict(int)\n",
    "for review in dataTrain:\n",
    "    text = preprocess_text(review['text'])\n",
    "    words = set(text.split())  # Unique words in the document\n",
    "    for w in words:\n",
    "        df[w] += 1\n",
    "\n",
    "N = len(dataTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IDF for target words\n",
    "idf = {}\n",
    "for word in target_words:\n",
    "    idf[word] = math.log10(N / (1 + df[word]))\n",
    "\n",
    "first_review = preprocess_text(dataTrain[0]['text'])\n",
    "first_review_words = first_review.split()\n",
    "\n",
    "tf = {}\n",
    "for word in target_words:\n",
    "    tf[word] = sum(1 for w in first_review_words if w == word)\n",
    "\n",
    "# Compute TF-IDF for the target words\n",
    "tfidf = {}\n",
    "for word in target_words:\n",
    "    tfidf[word] = tf[word] * idf[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = [(idf[word], tfidf[word]) for word in target_words]\n",
    "\n",
    "assertFloatList([x[0] for x in answers['Q3']], 5)\n",
    "assertFloatList([x[1] for x in answers['Q3']], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build X and y..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = X[:Ntrain]\n",
    "ytrain = y[:Ntrain]\n",
    "Xtest = X[Ntrain:]\n",
    "ytest = y[Ntrain:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=1)\n",
    "mod.fit(Xtrain, ytrain)\n",
    "\n",
    "predictions = mod.predict(Xtest)\n",
    "correct = [predictions[i] == ytest[i] for i in range(len(ytest))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = sum(correct) / len(correct)\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(x1, x2):\n",
    "    dot_product = sum(x1.get(word, 0) * x2.get(word, 0) for word in x1.keys() | x2.keys())\n",
    "    \n",
    "    norm_x1 = math.sqrt(sum(value**2 for value in x1.values()))\n",
    "    norm_x2 = math.sqrt(sum(value**2 for value in x2.values()))\n",
    "    \n",
    "    # edge cases\n",
    "    if norm_x1 == 0 or norm_x2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm_x1 * norm_x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IDF\n",
    "def compute_idf(dataTrain):\n",
    "    idf = defaultdict(float)\n",
    "    doc_count = len(dataTrain)\n",
    "    word_doc_count = Counter()\n",
    "\n",
    "    for review in dataTrain:\n",
    "        words = set(preprocess_text(review['text']).split())\n",
    "        for word in words:\n",
    "            word_doc_count[word] += 1\n",
    "\n",
    "    for word, count in word_doc_count.items():\n",
    "        idf[word] = math.log10(doc_count / count) if count > 0 else 0.0\n",
    "\n",
    "    return idf\n",
    "\n",
    "idf = compute_idf(dataTrain)\n",
    "\n",
    "def compute_tfidf(text):\n",
    "    words = preprocess_text(text).split()\n",
    "    tf = defaultdict(int)\n",
    "    for word in words:\n",
    "        tf[word] += 1\n",
    "    return {word: (tf[word] / len(words)) * idf.get(word, 0.0) for word in tf}\n",
    "\n",
    "# Check TF-IDF and similarity computation\n",
    "first_review_tfidf = compute_tfidf(dataTrain[0]['text'])\n",
    "test_tfidfs = [(compute_tfidf(review['text']), review.get('reviewID', None)) for review in dataTest]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "for test_tfidf, reviewID in test_tfidfs:\n",
    "    similarity = Cosine(first_review_tfidf, test_tfidf)\n",
    "    similarities.append((similarity, reviewID))\n",
    "\n",
    "similarities.sort(reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = similarities[0]\n",
    "assertFloat(answers['Q5'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define possible values for NW and C\n",
    "dictionary_sizes = [500, 1000, 2000]\n",
    "regularization_constants = [0.01, 0.1, 1, 10]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_NW = None\n",
    "best_C = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Iterate over different dict size\n",
    "for NW in dictionary_sizes:\n",
    "    words = [word for word, _ in counts[:NW]]\n",
    "    vectorizer = TfidfVectorizer(vocabulary = words)\n",
    "    X = vectorizer.fit_transform([review['text'].lower() for review in dataset])\n",
    "    y = [review['genreID'] for review in dataset]\n",
    "    \n",
    "    Xtrain = X[:Ntrain]\n",
    "    ytrain = y[:Ntrain]\n",
    "    Xtest = X[Ntrain:]\n",
    "    ytest = y[Ntrain:]\n",
    "\n",
    "    # Iterate over different C values\n",
    "    for C in regularization_constants:\n",
    "        mod = linear_model.LogisticRegression(C=C)\n",
    "        mod.fit(Xtrain, ytrain)\n",
    "\n",
    "        predictions = mod.predict(Xtest)\n",
    "        accuracy = accuracy_score(ytest, predictions)\n",
    "\n",
    "        # Update best parameters\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_NW = NW\n",
    "            best_C = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = best_accuracy\n",
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "f = gzip.open(\"young_adult_20000.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    d['datetime'] = dateutil.parser.parse(d['date_added'])\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewLists = defaultdict(list)\n",
    "for review in dataset:\n",
    "    user = review['user_id']\n",
    "    book = review['book_id']\n",
    "    reviewLists[user].append((review['datetime'], book))\n",
    "\n",
    "reviewLists = [sorted(reviews, key = lambda x: x[0]) for reviews in reviewLists.values()]\n",
    "reviewLists = [[book for _, book in reviews] for reviews in reviewLists]  # Keep only book IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "model5 = Word2Vec(reviewLists,\n",
    "                  min_count=1, # Words/items with fewer instances are discarded\n",
    "                  vector_size=5, # Model dimensionality\n",
    "                  window=3, # Window size\n",
    "                  sg=1) # Skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_book = reviewLists[0][0]\n",
    "res = model5.wv.most_similar(first_book, topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = res[:5]\n",
    "assertFloatList([x[1] for x in answers['Q7']], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw4.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
