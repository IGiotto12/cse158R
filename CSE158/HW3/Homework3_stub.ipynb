{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31cab31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f967ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e25a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f88efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a5f39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3b16eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09ac1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4717806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca3c2a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('u47877739', 'b50020691', 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u,b,r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))\n",
    "\n",
    "ratingsValid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93959f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abb17ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from baseline code\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort(reverse=True)\n",
    "\n",
    "return1 = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    count += ic\n",
    "    return1.add(i)\n",
    "    if count > totalRead/2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80f40789",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c9eea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = []\n",
    "for user, book, _ in ratingsValid:\n",
    "    # Add the positive sample\n",
    "    validation_set.append((user, book, 1))  # 1 indicates a read\n",
    "    \n",
    "    # Generate a negative sample (a book the user hasn’t read)\n",
    "    while True:\n",
    "        negative_book = random.choice(list(bookCount.keys()))\n",
    "        if negative_book not in [b for b, _ in ratingsPerUser[user]]:\n",
    "            validation_set.append((user, negative_book, 0))  # 0 indicates non-read\n",
    "            break\n",
    "\n",
    "# Evaluate the baseline model on this validation set\n",
    "correct_predictions = 0\n",
    "for user, book, actual in validation_set:\n",
    "    prediction = 1 if book in return1 else 0\n",
    "    if prediction == actual:\n",
    "        correct_predictions += 1\n",
    "\n",
    "acc1 = correct_predictions / len(validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8af7b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1'] = acc1\n",
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50491907",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87e03b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "acc2 = 0\n",
    "\n",
    "# Loop through different thresholds\n",
    "for threshold_percentage in range(20, 80, 10):\n",
    "    threshold_value = (threshold_percentage / 100) * totalRead\n",
    "    current_return1 = set()\n",
    "    count = 0\n",
    "    \n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        current_return1.add(i)\n",
    "        if count > threshold_value:\n",
    "            break\n",
    "    \n",
    "    # Calculate accuracy on the validation set\n",
    "    correct_predictions = 0\n",
    "    for user, book, actual in validation_set:\n",
    "        prediction = 1 if book in current_return1 else 0\n",
    "        if prediction == actual:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / len(validation_set)\n",
    "    \n",
    "    # Update best threshold if current accuracy is higher\n",
    "    if accuracy > acc2:\n",
    "        acc2 = accuracy\n",
    "        threshold = threshold_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "263c16a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [threshold, acc2]\n",
    "\n",
    "assertFloat(answers['Q2'][0])\n",
    "assertFloat(answers['Q2'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b753559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04a6f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Jaccard similarity function\n",
    "def jaccard_similarity(book1, book2):\n",
    "    users1 = set(u for u, _ in ratingsPerItem[book1])\n",
    "    users2 = set(u for u, _ in ratingsPerItem[book2])\n",
    "    intersection = len(users1 & users2)\n",
    "    union = len(users1 | users2)\n",
    "    return intersection / union if union else 0\n",
    "\n",
    "# Construct the validation set with clear labels\n",
    "validation_set = []\n",
    "for user, book, _ in ratingsValid:\n",
    "    # Add the positive sample\n",
    "    validation_set.append((user, book, 1))  # 1 indicates a read (positive sample)\n",
    "    \n",
    "    # Generate a negative sample (a book the user hasn’t read)\n",
    "    while True:\n",
    "        negative_book = random.choice(list(bookCount.keys()))\n",
    "        if negative_book not in [b for b, _ in ratingsPerUser[user]]:\n",
    "            validation_set.append((user, negative_book, 0))  # 0 indicates non-read (negative sample)\n",
    "            break\n",
    "\n",
    "# Precompute maximum Jaccard similarity for each (user, book) pair in the validation set\n",
    "jaccard_similarities = {}\n",
    "\n",
    "for user, book, actual in validation_set:\n",
    "    max_similarity = 0\n",
    "    \n",
    "    # Calculate the maximum Jaccard similarity with books the user has read\n",
    "    for b, _ in ratingsPerUser[user]:\n",
    "        similarity = jaccard_similarity(book, b)\n",
    "        max_similarity = max(max_similarity, similarity)\n",
    "    \n",
    "    # Store the max similarity for this (user, book) pair\n",
    "    jaccard_similarities[(user, book)] = max_similarity\n",
    "\n",
    "# Optimize the threshold for maximum accuracy\n",
    "best_jaccard_threshold = 0\n",
    "acc3 = 0\n",
    "\n",
    "# Loop through a range of threshold values to find the best threshold\n",
    "for threshold in [0.01, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.4, 0.5, 0.6, 0.7]:\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    for user, book, actual in validation_set:\n",
    "        max_similarity = jaccard_similarities[(user, book)]\n",
    "        \n",
    "        # Predict \"read\" if max similarity exceeds the threshold\n",
    "        prediction = 1 if max_similarity >= threshold else 0\n",
    "        if prediction == actual:\n",
    "            correct_predictions += 1\n",
    "    \n",
    "    # Calculate the accuracy for the current threshold\n",
    "    accuracy = correct_predictions / len(validation_set)\n",
    "    \n",
    "    # Update best threshold if current accuracy is higher\n",
    "    if accuracy > acc3:\n",
    "        acc3 = accuracy\n",
    "        best_jaccard_threshold = threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f03f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc4 = 0\n",
    "best_combined_jaccard_threshold = 0\n",
    "best_combined_popularity_threshold = 0\n",
    "\n",
    "# Loop through possible Jaccard thresholds\n",
    "for jaccard_threshold in [0.1, 0.2, 0.3, 0.4, 0.5]:\n",
    "    for pop_threshold_percentage in range(20, 80, 10):\n",
    "        threshold_value = (pop_threshold_percentage / 100) * totalRead\n",
    "        current_return1 = set()\n",
    "        count = 0\n",
    "        \n",
    "        for ic, i in mostPopular:\n",
    "            count += ic\n",
    "            current_return1.add(i)\n",
    "            if count > threshold_value:\n",
    "                break\n",
    "        \n",
    "        # Evaluate accuracy with the combined approach\n",
    "        correct_predictions = 0\n",
    "        for user, book, actual in validation_set:\n",
    "            is_popular = book in current_return1\n",
    "            \n",
    "            # Check max Jaccard similarity\n",
    "            max_similarity = 0\n",
    "            for b, _ in ratingsPerUser[user]:\n",
    "                similarity = jaccard_similarity(book, b)\n",
    "                max_similarity = max(max_similarity, similarity)\n",
    "            \n",
    "            # Predict read if either condition is met\n",
    "            prediction = 1 if is_popular or max_similarity >= jaccard_threshold else 0\n",
    "            if prediction == actual:\n",
    "                correct_predictions += 1\n",
    "        \n",
    "        accuracy = correct_predictions / len(validation_set)\n",
    "        \n",
    "        # Update best parameters if accuracy improves\n",
    "        if accuracy > acc4:\n",
    "            acc4 = accuracy\n",
    "            best_combined_jaccard_threshold = jaccard_threshold\n",
    "            best_combined_popularity_threshold = pop_threshold_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83ab0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = acc3\n",
    "answers['Q4'] = acc4\n",
    "\n",
    "assertFloat(answers['Q3'])\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e68cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "for l in open(\"pairs_Read.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    \n",
    "    # Check if the book is popular enough (based on best popularity threshold)\n",
    "    is_popular = b in return1\n",
    "    \n",
    "    # Check Jaccard similarity with books the user has read\n",
    "    max_similarity = 0\n",
    "    if u in ratingsPerUser:\n",
    "        for b_read, _ in ratingsPerUser[u]:\n",
    "            similarity = jaccard_similarity(b, b_read)\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "    \n",
    "    # Predict \"read\" based on combined model or best individual model\n",
    "    if is_popular or max_similarity >= best_combined_jaccard_threshold:\n",
    "        prediction = 1  # Predict \"read\"\n",
    "    else:\n",
    "        prediction = 0  # Predict \"non-read\"\n",
    "    \n",
    "    # Write the prediction result to the output file\n",
    "    predictions.write(f\"{u},{b},{prediction}\\n\")\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "297b5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\"\n",
    "assert type(answers['Q5']) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcf70975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Rating prediction                              #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95b960a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "af7f3f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters for rating(u, i)\n",
    "alpha = sum([r for _, _, r in ratingsTrain]) / len(ratingsTrain)  # Global average rating\n",
    "\n",
    "lambda_reg = 1\n",
    "beta_user = defaultdict(float)\n",
    "beta_item = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d69e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define gradient descent function for bias terms\n",
    "def gradient_descent(lr=0.0001, iterations=100):\n",
    "    for _ in range(iterations):\n",
    "        user_grad = defaultdict(float)\n",
    "        item_grad = defaultdict(float)\n",
    "\n",
    "        # Accumulate gradients for each user and item\n",
    "        for u, i, r in ratingsTrain:\n",
    "            prediction = alpha + beta_user[u] + beta_item[i]\n",
    "            error = r - prediction\n",
    "            \n",
    "            # Gradient calculation with regularization\n",
    "            user_grad[u] += -2 * error + 2 * lambda_reg * beta_user[u]\n",
    "            item_grad[i] += -2 * error + 2 * lambda_reg * beta_item[i]\n",
    "        \n",
    "        # Update bias terms with a cap to avoid large values\n",
    "        for u in user_grad:\n",
    "            beta_user[u] = max(min(beta_user[u] - lr * user_grad[u], 10), -10)\n",
    "        for i in item_grad:\n",
    "            beta_item[i] = max(min(beta_item[i] - lr * item_grad[i], 10), -10)\n",
    "\n",
    "# Run gradient descent with adjusted parameters\n",
    "gradient_descent()\n",
    "\n",
    "# Calculate MSE on validation set with controlled values\n",
    "validMSE = 0\n",
    "for u, i, r in ratingsValid:\n",
    "    prediction = alpha + beta_user[u] + beta_item[i]\n",
    "    validMSE += (r - prediction) ** 2\n",
    "\n",
    "validMSE /= len(ratingsValid) if len(ratingsValid) else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "422ab930",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6'] = validMSE\n",
    "assertFloat(answers['Q6'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9826cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0276b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the user with the largest beta_user value\n",
    "maxUser = max(beta_user, key=beta_user.get)\n",
    "maxBeta = beta_user[maxUser]\n",
    "\n",
    "# Find the user with the smallest (most negative) beta_user value\n",
    "minUser = min(beta_user, key=beta_user.get)\n",
    "minBeta = beta_user[minUser]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c61b675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = [maxUser, minUser, maxBeta, minBeta]\n",
    "assert [type(x) for x in answers['Q7']] == [str, str, float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a416949",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae54cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a range of lambda values\n",
    "lambda_values = [0.1, 0.5, 1, 5, 10]\n",
    "lamb = None\n",
    "validMSE = float('inf')\n",
    "\n",
    "# Initialize global average rating alpha based on training data\n",
    "alpha = sum([r for _, _, r in ratingsTrain]) / len(ratingsTrain)\n",
    "\n",
    "# Function to calculate MSE for a given lambda with regularization\n",
    "def calculate_mse_for_lambda(lambda_reg, lr=0.005, max_iter=100, tol=1e-4):\n",
    "    # Initialize bias terms\n",
    "    beta_user = defaultdict(float)\n",
    "    beta_item = defaultdict(float)\n",
    "    \n",
    "    # Gradient descent with decaying learning rate and convergence check\n",
    "    for iteration in range(max_iter):\n",
    "        user_grad = defaultdict(float)\n",
    "        item_grad = defaultdict(float)\n",
    "        \n",
    "        # Accumulate gradients for each user and item with regularization\n",
    "        for u, i, r in ratingsTrain:\n",
    "            prediction = alpha + beta_user[u] + beta_item[i]\n",
    "            error = r - prediction\n",
    "            \n",
    "            # Gradient calculation with L2 regularization\n",
    "            user_grad[u] += -2 * error + 2 * lambda_reg * beta_user[u]\n",
    "            item_grad[i] += -2 * error + 2 * lambda_reg * beta_item[i]\n",
    "        \n",
    "        # Update beta_user and beta_item with capped values\n",
    "        learning_rate = lr / (1 + 0.01 * iteration)\n",
    "        max_change = 0\n",
    "        \n",
    "        for u in user_grad:\n",
    "            update = learning_rate * user_grad[u]\n",
    "            beta_user[u] = max(min(beta_user[u] - update, 10), -10)  # Cap to [-10, 10]\n",
    "            max_change = max(max_change, abs(update))\n",
    "        \n",
    "        for i in item_grad:\n",
    "            update = learning_rate * item_grad[i]\n",
    "            beta_item[i] = max(min(beta_item[i] - update, 10), -10)  # Cap to [-10, 10]\n",
    "            max_change = max(max_change, abs(update))\n",
    "        \n",
    "        # Stop if updates are very small\n",
    "        if max_change < tol:\n",
    "            break\n",
    "\n",
    "    # Calculate MSE on validation set\n",
    "    mse = 0\n",
    "    for u, i, r in ratingsValid:\n",
    "        prediction = alpha + beta_user[u] + beta_item[i]\n",
    "        mse += (r - prediction) ** 2\n",
    "    \n",
    "    # Regularization term\n",
    "    reg_term = lambda_reg * (sum(v ** 2 for v in beta_user.values()) + sum(v ** 2 for v in beta_item.values()))\n",
    "    \n",
    "    # Final MSE with regularization\n",
    "    mse = mse / len(ratingsValid) + reg_term\n",
    "    \n",
    "    return mse\n",
    "\n",
    "# Loop through lambda values to find the best one based on MSE\n",
    "for lambda_reg in lambda_values:\n",
    "    mse = calculate_mse_for_lambda(lambda_reg)\n",
    "    if mse < validMSE:\n",
    "        validMSE = mse\n",
    "        lamb = lambda_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1880fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q8'] = (lamb, validMSE)\n",
    "\n",
    "assertFloat(answers['Q8'][0])\n",
    "assertFloat(answers['Q8'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9bd53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.csv\", 'w')\n",
    "for l in open(\"pairs_Rating.csv\"):\n",
    "    if l.startswith(\"userID\"): # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',') # Read the user and item from the \"pairs\" file and write out your prediction\n",
    "    # Prediction using the learned biases\n",
    "    prediction = alpha\n",
    "    if u in beta_user:\n",
    "        prediction += beta_user[u]\n",
    "    if b in beta_item:\n",
    "        prediction += beta_item[b]\n",
    "    \n",
    "    # Write the prediction to the file\n",
    "    predictions.write(f\"{u},{b},{prediction}\\n\")\n",
    "    \n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "839261ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
